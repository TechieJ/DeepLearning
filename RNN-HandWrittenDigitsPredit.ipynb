{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN - Recurrent Neural networks\n",
    "# Order of data carries significance or importance.\n",
    "# For ex - Timeseries data where data is organised temporarlly or chronologically.\n",
    "# Order is important in natural language\n",
    "# Idea is --- \n",
    "# X1 --> A1 ----> O1\n",
    "#        |\n",
    "# X2 --> A2 ----> O2\n",
    "#        |\n",
    "# X3 --> A3 ----> O3\n",
    "# A is Recurrent cells - Its like LSTM which is Long short term memory. Its also the GRU (gated recurrent unit).\n",
    "# Many people use LSTM cells.\n",
    "# X is sequential data\n",
    "# Every layer is output to another layer. But in RNN, it outputs to two locations. One to the next layer and down to the next node as well (another recurrent layer).\n",
    "# You can have bi-directional RNN\n",
    "# Suppose, first layer node A1 sends data to another RNN node A2, then A2 decides what its going to do with A2 data and X2 data and use it forget it.\n",
    "# Lets write our own basic RNN\n",
    "# Hard part is getting your dataset structured in such a way becasue type of data that you are going to be using typically doesnt have targets.\n",
    "# Its just timeseries, its just informal kind of data. There is huge amount of prerocessing involved with RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 158s 3ms/sample - loss: 0.6349 - accuracy: 0.7828 - val_loss: 0.2008 - val_accuracy: 0.9416\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 147s 2ms/sample - loss: 0.1636 - accuracy: 0.9564 - val_loss: 0.1514 - val_accuracy: 0.9607\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 147s 2ms/sample - loss: 0.1070 - accuracy: 0.9712 - val_loss: 0.0748 - val_accuracy: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e22aca808>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM #, CuDNNLSTM (If you have GPU version)\n",
    "# Cuda LSTM is 5 times faster. but this is also fine to use for nwo.\n",
    "# Cuda LSTM use tanh activation function.\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Scale the data between 0 and 1. Normalise it.\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add( LSTM( 128, input_shape=(x_train.shape[1:]), activation=\"relu\" , return_sequences=True) )\n",
    "# Do we want to layer to return sequences.\n",
    "# If we are going to send data to dense layer then we dont want it to return sequences becasue dense layer is not going to understand.\n",
    "# But if we are going to send to another Recurrent layer, then we need to return sequences. Like sequences were input.\n",
    "model.add(Dropout(0.2)) #20% dropout\n",
    "\n",
    "model.add( LSTM( 128, activation=\"relu\" ) ) # relu - rectified linear.\n",
    "model.add( Dropout(0.2) )\n",
    "\n",
    "model.add( Dense(32, activation=\"relu\") )\n",
    "model.add( Dropout(0.2) )\n",
    "\n",
    "model.add( Dense(10, activation=\"softmax\") )\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "# IF you take smaller steps you can do better.\n",
    "# Decay will decay the learning rate a little bit so its gonna keep shrinking the learning rate and you will take smaller steps.\n",
    "\n",
    "model.compile( loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"] )\n",
    "\n",
    "model.fit( x_train, y_train, epochs=3, validation_data=(x_test, y_test) )\n",
    "\n",
    "# Validation accuracy is higher than train accuracy.\n",
    "# train accuracy is average of entire epoch and val accuracy is at the end of the epoch\n",
    "# This tell that we should train it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
